{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tcU4e4mKW0wKvkudNJL_kwSDYp3CB7sD",
      "authorship_tag": "ABX9TyONnPO4nnX5acJJ/bLWLBEK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivpandey2406/NLP/blob/main/Extract_text_from_the_URL_and_calculate_specified_variables_from_the_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4WHuZ_u_iTe",
        "outputId": "0c3d595a-8ff8-4d04-b2d0-31dad65fde4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Attempt to extract main article text\n",
        "        # This is a generic approach; you can customize for specific sites\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return article_text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Load Excel file and URLs from column 'B'\n",
        "input_file = '/content/sample_data/Input.xlsx'  # Change to your file name\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Assuming column 'B' is the second column (index 1)\n",
        "url_column = df.columns[1]\n",
        "df['Extracted_Text'] = df[url_column].apply(extract_text_from_url)\n",
        "\n",
        "# Save results to a new Excel file\n",
        "output_file = 'output_with_articles.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "print(f\"Extraction complete. Results saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90HtTgaZ_pD3",
        "outputId": "5d21fe7e-7b9c-439e-d604-cd3e0fe51474"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete. Results saved to output_with_articles.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas textblob nltk openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2f4Opd_CAQM",
        "outputId": "783ed21e-046c-4e0e-b7e6-fcffeb03935c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjymk8_ICDVS",
        "outputId": "7616c8d8-ffa6-450e-97b4-d6027b007f89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Attempt to extract main article text\n",
        "        # This is a generic approach; you can customize for specific sites\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return article_text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def load_word_set(filepath):\n",
        "    # Try opening with utf-8 first\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            words = set(line.strip() for line in f if line.strip() and not line.startswith(';'))\n",
        "        return words\n",
        "    except UnicodeDecodeError:\n",
        "        # If utf-8 fails, try latin-1\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='latin-1') as f:\n",
        "                 words = set(line.strip() for line in f if line.strip() and not line.startswith(';'))\n",
        "            print(f\"Successfully loaded {filepath} with latin-1 encoding.\")\n",
        "            return words\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filepath} with latin-1 encoding: {e}\")\n",
        "            # Handle or re-raise the error if other encodings fail\n",
        "            raise e\n",
        "\n",
        "\n",
        "positive_words = load_word_set('/content/sample_data/positive-words.txt')\n",
        "negative_words = load_word_set('/content/sample_data/negative-words.txt')\n",
        "\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    vowels = \"aeiouy\"\n",
        "    count = 0\n",
        "    prev_char_was_vowel = False\n",
        "    for char in word:\n",
        "        if char in vowels:\n",
        "            if not prev_char_was_vowel:\n",
        "                count += 1\n",
        "            prev_char_was_vowel = True\n",
        "        else:\n",
        "            prev_char_was_vowel = False\n",
        "    if word.endswith(\"e\"):\n",
        "        count = max(1, count-1)\n",
        "    return max(count, 1)\n",
        "\n",
        "def is_complex(word):\n",
        "    return count_syllables(word) > 2\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    # Simple regex for personal pronouns\n",
        "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
        "    return len(pronouns)\n",
        "\n",
        "def text_metrics(text):\n",
        "    # Handle non-string input gracefully\n",
        "    if not isinstance(text, str):\n",
        "        # Return a series of zeros or NaNs for non-string inputs\n",
        "        return pd.Series([0] * 13, index=[\n",
        "            \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "            \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
        "            \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
        "            \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
        "        ])\n",
        "\n",
        "    blob = TextBlob(text)\n",
        "    sentences = blob.sentences\n",
        "    words = blob.words\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "    if sentence_count == 0: sentence_count = 1\n",
        "\n",
        "    # Positive/Negative Score\n",
        "    pos_score = sum(1 for w in words if w.lower() in positive_words)\n",
        "    neg_score = sum(1 for w in words if w.lower() in negative_words)\n",
        "\n",
        "    # Polarity/Subjectivity\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "    # Average sentence length\n",
        "    avg_sent_len = word_count / sentence_count\n",
        "\n",
        "    # Complex words\n",
        "    complex_words = [w for w in words if is_complex(w)]\n",
        "    complex_word_count = len(complex_words)\n",
        "    percent_complex = (complex_word_count / word_count) * 100 if word_count else 0\n",
        "\n",
        "    # Fog Index\n",
        "    fog_index = 0.4 * (avg_sent_len + percent_complex)\n",
        "\n",
        "    # Avg words per sentence\n",
        "    avg_words_per_sentence = avg_sent_len\n",
        "\n",
        "    # Syllables per word\n",
        "    syllable_per_word = sum(count_syllables(w) for w in words) / word_count if word_count else 0\n",
        "\n",
        "    # Personal pronouns\n",
        "    personal_pronouns = count_personal_pronouns(text)\n",
        "\n",
        "    # Avg word length\n",
        "    avg_word_len = sum(len(w) for w in words) / word_count if word_count else 0\n",
        "\n",
        "    return pd.Series([\n",
        "        pos_score, neg_score, polarity, subjectivity, avg_sent_len, percent_complex,\n",
        "        fog_index, avg_words_per_sentence, complex_word_count, word_count, syllable_per_word,\n",
        "        personal_pronouns, avg_word_len\n",
        "    ])\n",
        "\n",
        "# Read Excel\n",
        "input_file = '/content/sample_data/output_with_articles.xlsx'  # Change to your file\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Apply metrics to column C (index 2)\n",
        "# Ensure the column is treated as strings before applying text_metrics\n",
        "df.iloc[:, 2] = df.iloc[:, 2].astype(str)\n",
        "metrics = df.iloc[:, 2].apply(text_metrics)\n",
        "metrics.columns = [\n",
        "    \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
        "    \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
        "    \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "# Concatenate and save\n",
        "result = pd.concat([df, metrics], axis=1)\n",
        "result.to_excel('output_with_metrics.xlsx', index=False)\n",
        "print(\"Analysis complete. Results saved to output_with_metrics.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqhSZe-GQktw",
        "outputId": "600ce565-60ef-460a-f2f2-8852839b3bbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded /content/sample_data/negative-words.txt with latin-1 encoding.\n",
            "Analysis complete. Results saved to output_with_metrics.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sQMuDsJDK-3",
        "outputId": "a8191ac5-b802-456e-a6bb-01041caf087f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "from textblob import TextBlob # Still used for sentence splitting and tokenization\n",
        "from nltk.corpus import stopwords # Import stopwords\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    # This function remains the same as before, responsible for fetching text\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return article_text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def load_word_set(filepath):\n",
        "    # This function remains the same, loads positive/negative words\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            words = set(line.strip() for line in f if line.strip() and not line.startswith(';'))\n",
        "        return words\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='latin-1') as f:\n",
        "                 words = set(line.strip() for line in f if line.strip() and not line.startswith(';'))\n",
        "            print(f\"Successfully loaded {filepath} with latin-1 encoding.\")\n",
        "            return words\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filepath} with latin-1 encoding: {e}\")\n",
        "            raise e\n",
        "\n",
        "# Load your positive and negative word lists\n",
        "positive_words = load_word_set('/content/sample_data/positive-words.txt')\n",
        "negative_words = load_word_set('/content/sample_data/negative-words.txt')\n",
        "\n",
        "def count_syllables(word):\n",
        "    # Adjusted syllable count based on common rules, including 'es' and 'ed'\n",
        "    word = word.lower()\n",
        "    # Remove non-alphabetic characters for syllable counting\n",
        "    word = re.sub(r'[^a-z]', '', word)\n",
        "\n",
        "    if len(word) == 0:\n",
        "        return 0\n",
        "\n",
        "    count = 0\n",
        "    vowels = \"aeiouy\"\n",
        "    # Handle words ending with 'es' or 'ed'\n",
        "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
        "         # Check if the letter before 'es' or 'ed' is a vowel.\n",
        "         # If it is, these endings often add a syllable.\n",
        "         # If it is not, these endings often do not add a syllable.\n",
        "         # This is a simplification; actual rules are more complex.\n",
        "         # Let's count vowels and then adjust for these endings.\n",
        "         pass # Handle adjustment after vowel counting\n",
        "\n",
        "    prev_char_was_vowel = False\n",
        "    for i in range(len(word)):\n",
        "        if word[i] in vowels:\n",
        "            if not prev_char_was_vowel:\n",
        "                count += 1\n",
        "            prev_char_was_vowel = True\n",
        "        else:\n",
        "            prev_char_was_vowel = False\n",
        "\n",
        "    # Adjust for silent 'e' at the end of a word, unless it's a single-letter word 'e'\n",
        "    if word.endswith(\"e\") and not prev_char_was_vowel and len(word) > 1:\n",
        "        count = max(1, count - 1)\n",
        "\n",
        "    # Adjust for 'es' and 'ed' endings\n",
        "    if len(word) > 2: # Ensure the word is long enough for these endings to matter\n",
        "        if word.endswith(\"es\"):\n",
        "             # If the letter before 'es' is s, x, z, ch, sh, or ge, it usually adds a syllable (passes, foxes, buzzes, etc.)\n",
        "             # Otherwise, it often doesn't (likes)\n",
        "             if word[-3] in 'sxz' or word[-4:-2] in ('ch', 'sh') or word.endswith('ge'):\n",
        "                 pass # Don't subtract, assumes vowel counting already got it right or adds one\n",
        "             else:\n",
        "                 count = max(1, count - 1) # Often silent 'e' rule applies here, but 'es' adds no sound.\n",
        "        elif word.endswith(\"ed\"):\n",
        "            # If the preceding letter is 't' or 'd', it adds a syllable (waited, ended)\n",
        "            # Otherwise, it often doesn't (liked, played)\n",
        "            if word[-3] in 'td':\n",
        "                pass # Don't subtract, assumes vowel counting got it right\n",
        "            else:\n",
        "                 count = max(1, count - 1) # Often silent 'e' rule applies here, but 'ed' adds no sound.\n",
        "\n",
        "\n",
        "    # Ensure a word has at least one syllable\n",
        "    return max(count, 1)\n",
        "\n",
        "\n",
        "def is_complex(word):\n",
        "    # Complex words have more than two syllables\n",
        "    return count_syllables(word) > 2\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    # Use a slightly refined regex to avoid matching \"US\" as a country\n",
        "    # \\b ensures word boundaries. Negative lookbehind (?<!\\bUS) asserts that it's not preceded by '\\bUS'.\n",
        "    pronouns = re.findall(r'\\b(I|we|my|ours|us)(?<!\\bUS)\\b', text, re.I)\n",
        "    return len(pronouns)\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize into words\n",
        "    words = text.split()\n",
        "    # Remove stop words\n",
        "    cleaned_words = [word for word in words if word not in stop_words]\n",
        "    return cleaned_words\n",
        "\n",
        "def text_metrics(text):\n",
        "    # Handle non-string input gracefully\n",
        "    if not isinstance(text, str):\n",
        "        return pd.Series([0] * 13, index=[\n",
        "            \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "            \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
        "            \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
        "            \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
        "        ])\n",
        "\n",
        "    # Use TextBlob for initial sentence tokenization\n",
        "    blob = TextBlob(text)\n",
        "    sentences = blob.sentences\n",
        "    sentence_count = len(sentences)\n",
        "    if sentence_count == 0: sentence_count = 1 # Avoid division by zero\n",
        "\n",
        "    # Clean text for word count and sentiment analysis based on your criteria\n",
        "    cleaned_words = clean_text(text)\n",
        "    total_cleaned_word_count = len(cleaned_words)\n",
        "\n",
        "    # Recalculate word count for other metrics (including stopwords and punctuation for sentence length, complex words, etc.)\n",
        "    # Tokenize using TextBlob's word tokenization for metrics like avg word length, complex word count\n",
        "    all_words = blob.words\n",
        "\n",
        "    # Positive/Negative Score based on cleaned words\n",
        "    pos_score = sum(1 for w in cleaned_words if w in positive_words)\n",
        "    neg_score = sum(1 for w in cleaned_words if w in negative_words)\n",
        "\n",
        "    # Polarity Score based on your formula\n",
        "    denominator_polarity = pos_score + neg_score + 0.000001\n",
        "    polarity = (pos_score - neg_score) / denominator_polarity if denominator_polarity else 0\n",
        "\n",
        "    # Subjectivity Score based on your formula\n",
        "    denominator_subjectivity = total_cleaned_word_count + 0.000001\n",
        "    subjectivity = (pos_score + neg_score) / denominator_subjectivity if denominator_subjectivity else 0\n",
        "\n",
        "    # Average sentence length (based on total words including stopwords, before punctuation removal)\n",
        "    # This aligns better with a readability index calculation.\n",
        "    total_words_for_avg_sentence_length = len(all_words)\n",
        "    avg_sent_len = total_words_for_avg_sentence_length / sentence_count if sentence_count else 0\n",
        "\n",
        "    # Complex words (based on all words, before cleaning, for readability)\n",
        "    complex_words = [w for w in all_words if is_complex(w)]\n",
        "    complex_word_count = len(complex_words)\n",
        "    # Percentage of Complex words (based on all words)\n",
        "    percent_complex = (complex_word_count / total_words_for_avg_sentence_length) * 100 if total_words_for_avg_sentence_length else 0\n",
        "\n",
        "    # Fog Index\n",
        "    fog_index = 0.4 * (avg_sent_len + percent_complex)\n",
        "\n",
        "    # Avg words per sentence (same as Average Sentence Length based on total words)\n",
        "    avg_words_per_sentence = avg_sent_len\n",
        "\n",
        "    # Syllable Count Per Word (Average) - based on all words\n",
        "    syllable_per_word = sum(count_syllables(w) for w in all_words) / total_words_for_avg_sentence_length if total_words_for_avg_sentence_length else 0\n",
        "\n",
        "    # Personal pronouns (based on original text)\n",
        "    personal_pronouns = count_personal_pronouns(text)\n",
        "\n",
        "    # Average Word Length (based on all words, characters before cleaning)\n",
        "    avg_word_len = sum(len(w) for w in all_words) / total_words_for_avg_sentence_length if total_words_for_avg_sentence_length else 0\n",
        "\n",
        "\n",
        "    return pd.Series([\n",
        "        pos_score, neg_score, polarity, subjectivity, avg_sent_len, percent_complex,\n",
        "        fog_index, avg_words_per_sentence, complex_word_count, total_cleaned_word_count, # Use cleaned word count here as specified\n",
        "        syllable_per_word, personal_pronouns, avg_word_len\n",
        "    ])\n",
        "\n",
        "# --- Remainder of the code for loading, applying, and saving ---\n",
        "\n",
        "# Read Excel\n",
        "input_file = '/content/sample_data/output_with_articles.xlsx'  # Change to your file\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Apply metrics to the column containing the extracted text (assuming it's the 3rd column, index 2)\n",
        "text_column_index = 2\n",
        "df.iloc[:, text_column_index] = df.iloc[:, text_column_index].astype(str)\n",
        "metrics = df.iloc[:, text_column_index].apply(text_metrics)\n",
        "metrics.columns = [\n",
        "    \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
        "    \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\", # This is now the cleaned word count\n",
        "    \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "# Concatenate and save\n",
        "result = pd.concat([df, metrics], axis=1)\n",
        "output_file = 'output_with_metrics.xlsx'\n",
        "result.to_excel(output_file, index=False)\n",
        "print(f\"Analysis complete. Results saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcAkKjrZWS4j",
        "outputId": "78452784-ab4f-490a-a887-1812d858ab84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded /content/sample_data/negative-words.txt with latin-1 encoding.\n",
            "Analysis complete. Results saved to output_with_metrics.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "from textblob import TextBlob # Still used for sentence splitting and tokenization\n",
        "from nltk.corpus import stopwords # Import stopwords\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    # This function remains the same as before, responsible for fetching text\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        return article_text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def load_word_set(filepath):\n",
        "    # This function remains the same, loads positive/negative words\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            words = set(line.strip() for line in f if line.strip() and not line.startswith(';'))\n",
        "        return words\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='latin-1') as f:\n",
        "                 words = set(line.strip() for line in f if line.strip() and not line.startswith(';'))\n",
        "            print(f\"Successfully loaded {filepath} with latin-1 encoding.\")\n",
        "            return words\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filepath} with latin-1 encoding: {e}\")\n",
        "            raise e\n",
        "\n",
        "# Load your positive and negative word lists\n",
        "positive_words = load_word_set('/content/sample_data/positive-words.txt')\n",
        "negative_words = load_word_set('/content/sample_data/negative-words.txt')\n",
        "\n",
        "def count_syllables(word):\n",
        "    # Adjusted syllable count based on common rules, including 'es' and 'ed'\n",
        "    word = word.lower()\n",
        "    # Remove non-alphabetic characters for syllable counting\n",
        "    word = re.sub(r'[^a-z]', '', word)\n",
        "\n",
        "    if len(word) == 0:\n",
        "        return 0\n",
        "\n",
        "    count = 0\n",
        "    vowels = \"aeiouy\"\n",
        "    # Handle words ending with 'es' or 'ed'\n",
        "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
        "         # Check if the letter before 'es' or 'ed' is a vowel.\n",
        "         # If it is, these endings often add a syllable.\n",
        "         # If it is not, these endings often do not add a syllable.\n",
        "         # This is a simplification; actual rules are more complex.\n",
        "         # Let's count vowels and then adjust for these endings.\n",
        "         pass # Handle adjustment after vowel counting\n",
        "\n",
        "    prev_char_was_vowel = False\n",
        "    for i in range(len(word)):\n",
        "        if word[i] in vowels:\n",
        "            if not prev_char_was_vowel:\n",
        "                count += 1\n",
        "            prev_char_was_vowel = True\n",
        "        else:\n",
        "            prev_char_was_vowel = False\n",
        "\n",
        "    # Adjust for silent 'e' at the end of a word, unless it's a single-letter word 'e'\n",
        "    if word.endswith(\"e\") and not prev_char_was_vowel and len(word) > 1:\n",
        "        count = max(1, count - 1)\n",
        "\n",
        "    # Adjust for 'es' and 'ed' endings\n",
        "    if len(word) > 2: # Ensure the word is long enough for these endings to matter\n",
        "        if word.endswith(\"es\"):\n",
        "             # If the letter before 'es' is s, x, z, ch, sh, or ge, it usually adds a syllable (passes, foxes, buzzes, etc.)\n",
        "             # Otherwise, it often doesn't (likes)\n",
        "             if word[-3] in 'sxz' or word[-4:-2] in ('ch', 'sh') or word.endswith('ge'):\n",
        "                 pass # Don't subtract, assumes vowel counting already got it right or adds one\n",
        "             else:\n",
        "                 count = max(1, count - 1) # Often silent 'e' rule applies here, but 'es' adds no sound.\n",
        "        elif word.endswith(\"ed\"):\n",
        "            # If the preceding letter is 't' or 'd', it adds a syllable (waited, ended)\n",
        "            # Otherwise, it often doesn't (liked, played)\n",
        "            if word[-3] in 'td':\n",
        "                pass # Don't subtract, assumes vowel counting got it right\n",
        "            else:\n",
        "                 count = max(1, count - 1) # Often silent 'e' rule applies here, but 'ed' adds no sound.\n",
        "\n",
        "\n",
        "    # Ensure a word has at least one syllable\n",
        "    return max(count, 1)\n",
        "\n",
        "\n",
        "def is_complex(word):\n",
        "    # Complex words have more than two syllables\n",
        "    return count_syllables(word) > 2\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    # Use a slightly refined regex to avoid matching \"US\" as a country\n",
        "    # \\b ensures word boundaries. Negative lookbehind (?<!\\bUS) asserts that it's not preceded by '\\bUS'.\n",
        "    pronouns = re.findall(r'\\b(I|we|my|ours|us)(?<!\\bUS)\\b', text, re.I)\n",
        "    return len(pronouns)\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize into words\n",
        "    words = text.split()\n",
        "    # Remove stop words\n",
        "    cleaned_words = [word for word in words if word not in stop_words]\n",
        "    return cleaned_words\n",
        "\n",
        "def text_metrics(text):\n",
        "    # Handle non-string input gracefully\n",
        "    if not isinstance(text, str):\n",
        "        return pd.Series([0] * 13, index=[\n",
        "            \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "            \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
        "            \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
        "            \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
        "        ])\n",
        "\n",
        "    # Use TextBlob for initial sentence tokenization\n",
        "    blob = TextBlob(text)\n",
        "    sentences = blob.sentences\n",
        "    sentence_count = len(sentences)\n",
        "    if sentence_count == 0: sentence_count = 1 # Avoid division by zero\n",
        "\n",
        "    # Clean text for word count and sentiment analysis based on your criteria\n",
        "    cleaned_words = clean_text(text)\n",
        "    total_cleaned_word_count = len(cleaned_words)\n",
        "\n",
        "    # Recalculate word count for other metrics (including stopwords and punctuation for sentence length, complex words, etc.)\n",
        "    # Tokenize using TextBlob's word tokenization for metrics like avg word length, complex word count\n",
        "    all_words = blob.words\n",
        "\n",
        "    # Positive/Negative Score based on cleaned words\n",
        "    pos_score = sum(1 for w in cleaned_words if w in positive_words)\n",
        "    neg_score = sum(1 for w in cleaned_words if w in negative_words)\n",
        "\n",
        "    # Polarity Score based on your formula\n",
        "    polarity = (pos_score - neg_score) / ((pos_score + neg_score )+ 0.000001)\n",
        "\n",
        "    # Subjectivity Score based on your formula\n",
        "    subjectivity = (pos_score + neg_score) / ((total_cleaned_word_count) + 0.000001)\n",
        "\n",
        "    # Average sentence length (based on total words including stopwords, before punctuation removal)\n",
        "    # This aligns better with a readability index calculation.\n",
        "    total_words_for_avg_sentence_length = len(all_words)\n",
        "    avg_sent_len = total_words_for_avg_sentence_length / sentence_count if sentence_count else 0\n",
        "\n",
        "    # Complex words (based on all words, before cleaning, for readability)\n",
        "    complex_words = [w for w in all_words if is_complex(w)]\n",
        "    complex_word_count = len(complex_words)\n",
        "    # Percentage of Complex words (based on all words)\n",
        "    percent_complex = (complex_word_count / total_words_for_avg_sentence_length) * 100 if total_words_for_avg_sentence_length else 0\n",
        "\n",
        "    # Fog Index\n",
        "    fog_index = 0.4 * (avg_sent_len + percent_complex)\n",
        "\n",
        "    # Avg words per sentence (same as Average Sentence Length based on total words)\n",
        "    avg_words_per_sentence = avg_sent_len\n",
        "\n",
        "    # Syllable Count Per Word (Average) - based on all words\n",
        "    syllable_per_word = sum(count_syllables(w) for w in all_words) / total_words_for_avg_sentence_length if total_words_for_avg_sentence_length else 0\n",
        "\n",
        "    # Personal pronouns (based on original text)\n",
        "    personal_pronouns = count_personal_pronouns(text)\n",
        "\n",
        "    # Average Word Length (based on all words, characters before cleaning)\n",
        "    avg_word_len = sum(len(w) for w in all_words) / total_words_for_avg_sentence_length if total_words_for_avg_sentence_length else 0\n",
        "\n",
        "\n",
        "    return pd.Series([\n",
        "        pos_score, neg_score, polarity, subjectivity, avg_sent_len, percent_complex,\n",
        "        fog_index, avg_words_per_sentence, complex_word_count, total_cleaned_word_count, # Use cleaned word count here as specified\n",
        "        syllable_per_word, personal_pronouns, avg_word_len\n",
        "    ])\n",
        "\n",
        "# --- Remainder of the code for loading, applying, and saving ---\n",
        "\n",
        "# Read Excel\n",
        "input_file = '/content/sample_data/output_with_articles.xlsx'  # Change to your file\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Apply metrics to the column containing the extracted text (assuming it's the 3rd column, index 2)\n",
        "text_column_index = 2\n",
        "df.iloc[:, text_column_index] = df.iloc[:, text_column_index].astype(str)\n",
        "metrics = df.iloc[:, text_column_index].apply(text_metrics)\n",
        "metrics.columns = [\n",
        "    \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
        "    \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\", # This is now the cleaned word count\n",
        "    \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "# Concatenate and save\n",
        "result = pd.concat([df, metrics], axis=1)\n",
        "output_file = 'output_with_metrics.xlsx'\n",
        "result.to_excel(output_file, index=False)\n",
        "print(f\"Analysis complete. Results saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w95XcR7PYQhr",
        "outputId": "680e2ec5-e033-4367-9dd5-627eae8ea84c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded /content/sample_data/negative-words.txt with latin-1 encoding.\n",
            "Analysis complete. Results saved to output_with_metrics.xlsx\n"
          ]
        }
      ]
    }
  ]
}